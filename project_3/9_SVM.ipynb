{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Популярность метода опорных векторов (support-vector machines, SVM) сильно выросла за последние годы благодаря его высокой ошибкоустойчивости при классификации, в том числе в пространствах высокой размерности. Как ни удивительно, SVM работает, даже если количество измерений (признаков) превышает количество элементов данных, что необычно для алгоритмов классификации из-за так называемого проклятия размерности: при росте размерности данные становятся очень разреженными, а это усложняет поиск закономерностей в наборе данных. Понимание основных идей SVM — необходимый шаг становления опытного специалиста по машинному обучению.\n",
    "Как функционируют алгоритмы классификации? На основе обучающих данных они ищут границу решений, отделяющую данные из одного класса от данных из другого\n",
    "Пусть нам нужно создать рекомендательную систему для студентов начальных курсов университета.состоящие из пользователей, классифицированных по их способностям в двух сферах: логика и творчество. Одни студенты отличаются сильными логическими способностями и относительно низким уровнем творческих; другие — выраженными творческими способностями и относительно низким уровнем логических. Первую группу мы обозначили как специалисты по computer science, а вторую — представители искусства\n",
    "Для классификации новых пользователей модель машинного обучения должна отыскать границу решений, разделяющую специалистов по computer science и представителей искусства. В общих чертах будем классифицировать пользователей в зависимости от того, по какую сторону границы решений они попадают. В нашем примере мы классифицируем пользователей слева от границы решений как специалистов по computer science, а справа — как представителей искусства\n",
    "В двумерном пространстве роль границы решений может играть либо прямая, либо кривая (более высокого порядка). В первом случае классификатор называется линейным (linear classifier), а во втором — нелинейным (nonlinear classifier). В этом разделе мы будем рассматривать только линейные классификаторы.\n",
    "Метод опорных векторов дает уникальный и очень красивый ответ на этот вопрос. Вполне логично, что лучшая граница решений — та, которая обеспечивает максимальный «запас прочности». Другими словами, метод опорных векторов максимизирует расстояние между границей решений и ближайшими точками данных. Цель состоит в минимизации погрешности для новых точек, близких к границе решений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['computer science' 'computer science' 'literature' 'literature' 'art'\n",
      " 'art']\n",
      "['art']\n",
      "['computer science']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import svm \n",
    "\n",
    "## Данные: оценки студентов по (математика, языки, творческие\n",
    "## способности) --> предмет для изучения\n",
    "X = np.array([[9, 5, 6, \"computer science\"],\n",
    "[10, 1, 2, \"computer science\"],\n",
    "[1, 8, 1, \"literature\"],\n",
    "[4, 9, 3, \"literature\"],\n",
    "[0, 1, 10, \"art\"],\n",
    "[5, 7, 9, \"art\"]])\n",
    "\n",
    "svm = svm.SVC().fit(X[:, :-1], X[:, -1])\n",
    "# сначала подаем целочисленные данные[:, :-1], \n",
    "# после связываем с переменными [:, -1]\n",
    "# супер пупер удобная штука эти ваши срезы\n",
    "stu_0 = svm.predict([[3, 3, 6]])\n",
    "print(stu_0)\n",
    "\n",
    "stu_1 = svm.predict([[8, 1, 1]])\n",
    "print(stu_1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "векторов на Python. В массиве NumPy содержатся маркированные обучающие данные, по одной строке на пользователя и одному столбцу на признак (способности студентов к математике, языкам и творческие способности). Последний столбец — метка (класс).\n",
    "Поскольку данные у нас — трехмерные, метод опорных векторов разделяет их с помощью двумерных плоскостей (линейный разделитель), а не одномерных прямых. Как вы, наверное, видите, можно также разделять три класса, а не два, как в предыдущих примерах.\n",
    "Сам однострочник очень прост: сначала мы создаем модель с помощью конструктора класса svm.SVC (SVC расшифровывается как support-vector classification — классификация с помощью опорных векторов). Далее мы вызываем функцию fit(), производящую обучение на основе наших маркированных обучающих данных.\n",
    "В части «Результат» фрагмента кода мы вызываем функцию predict(), передавая ей новые наблюдения. Поскольку для student_0 указано\n",
    "математика = 3, языки = 3 и творческие способности = 6, то метод опорных векторов предсказывает, что способностям студента соответствует метка art. Аналогично, для student_1 с математика = 8, языки = 1 и творческие способности = 1 метод опорных векторов предсказывает, что способностям студента соответствует метка computer science."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Резюмируя: SVM демонстрирует хорошие результаты даже в многомерных пространствах при количестве признаков, превышающем количество обучающих векторов данных. Идея максимизации «запаса прочности» вполне интуитивна и демонстрирует хорошие результаты даже при классификации граничных случаев (boundary cases) — векторов, попадающих в рамки этого «запаса прочности». "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
