{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В предыдущих разделах мы изучили несколько алгоритмов машинного обучения, с помощью которых можно быстро получить неплохие результаты.\n",
    "Однако у разных алгоритмов — разные сильные стороны. Например, основанные на нейронных сетях классификаторы способны давать великолепные результаты для сложных задач, однако подвержены риску переобучения именно вследствие своих потрясающих способностей к усвоению тонких закономерностей данных. Обучение ансамблей для задач классификации частично решает проблему, связанную с тем, что заранее неизвестно, какой алгоритм машинного обучения сработает лучше всего.\n",
    "Как работает этот подход? Создается метаклассификатор, состоящий из нескольких типов или экземпляров простых алгоритмов машинного обучения. Другими словами, обучается несколько моделей. В целях классификации конкретного наблюдения входные данные передаются по отдельности всем моделям. А роль метапредсказания играет класс, который эти модели возвращали чаще всего при этих входных данных. Он и становится итоговым результатом алгоритма обучения ансамблей.\n",
    "Случайные леса (random forests) — особая разновидность алгоритмов обучения ансамблей, использующая обучение на основе деревьев принятия решений. Лес состоит из множества деревьев. Аналогично, случайный лес состоит из множества деревьев принятия решений. Отдельные деревья принятия решений получаются путем внесения стохастичности в процесс генерации деревьев на этапе обучения (например, выбор в качестве первой различных вершин дерева). В результате получаются различные деревья принятия решений — как раз то, что нужно. У Алисы выраженные способности к математике и языкам. Ансамбль состоит из трех деревьев принятия решений (составляющих случайный лес). Чтобы классифицировать Алису, мы просим все эти деревья ее классифицировать. Два из трех деревьев классифицируют Алису как специалиста по computer science. Этот класс как получивший максимум «голосов» и возвращается в качестве окончательного результата классификации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Продолжим работать с этим примером классификации изучаемых предметов на основе демонстрируемых студентами способностей в трех областях: математика, языки и творчество. Возможно, вам кажется, что реализовать метод обучения ансамблей на Python непросто. Но благодаря многогранности библиотеки scikit-learn это не так"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['computer science' 'art' 'art']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "## Данные: оценки студентов по (математика, языки, творческие\n",
    "## способности) --> предмет для изучения\n",
    "X = np.array([[9, 5, 6, \"computer science\"],\n",
    "[5, 1, 5, \"computer science\"],\n",
    "[8, 8, 8, \"computer science\"],\n",
    "[1, 10, 7, \"literature\"],\n",
    "[1, 8, 1, \"literature\"],\n",
    "[5, 7, 9, \"art\"],\n",
    "[1, 1, 6, \"art\"]])\n",
    "\n",
    "ft = RandomForestClassifier(n_estimators=10, random_state=1).fit(X[:,:-1], X[:,-1])\n",
    "\n",
    "stud = ft.predict([[8, 6, 5],\n",
    "                   [3, 7, 9],\n",
    "                   [2, 2, 1]])\n",
    "print(stud)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Инициализировав массив маркированных обучающих данных, код создает случайный лес с помощью конструктора класса RandomForestClassifier с одним параметром — n_estimators, — задающим количество деревьев в лесу. Далее мы вызываем функцию fit(), заполняя данными полученную при инициализации модель (пустой лес). Используемые для этого входные обучающие данные состоят из всех столбцов массива X, кроме последнего, а метки обучающих данных задаются в этом последнем столбце. Как и в предыдущих примерах, соответствующие столбцы из массива данных X мы выделяем с помощью срезов.\n",
    "Относящаяся к классификации часть этого фрагмента кода несколько отличается. Я хотел показать вам, как классифицировать много наблюдений, а не только одно. Это можно сделать тут путем создания многомерного массива, в котором каждому наблюдению соответствует одна строка.\n",
    "Вот результаты работы нашего фрагмента кода:\n",
    "Результат\n",
    "students = Forest.predict([[8, 6, 5],\n",
    "[3, 7, 9],\n",
    "[2, 2, 1]])\n",
    "print(students)\n",
    "['computer science' 'art' 'art']\n",
    "Обратите внимание, что результаты по-прежнему недетерминистичны (могут отличаться при различных запусках этого кода), поскольку в алгоритме случайных лесов используется генератор случайных чисел, возвращающий различные числа в различные моменты времени. Детерминизировать этот вызов можно с помощью целочисленного аргумента random_state. Например, можно задать параметр random_state=1 при вызове конструктора случайного леса: RandomForestClassifier(n_estimators=10, random_state=1). В этом случае при каждом создании классификатора на основе случайных лесов будут возвращаться одни и те же результаты, поскольку будут генерироваться одни и те же случайные числа: в основе их всех лежит начальное значение генератора 1.\n",
    "Резюмируя: в этом разделе представлен метаподход к классификации: снижение дисперсии погрешности классификации за счет использования результатов работы нескольких различных деревьев решений — одна из версий обучения ансамблей, при котором несколько базовых моделей объединяется в одну метамодель, способную задействовать все сильные стороны каждой из них.\n",
    "ПРИМЕЧАНИЕ\n",
    "Использование двух различных деревьев принятия решений может привести к высокой дисперсии погрешности, если одно возвращает хорошие результаты, а второе — нет. Последствия этого эффекта можно уменьшить с помощью случайных лесов.\n",
    "Различные вариации этой идеи очень распространены в машинном обучении. Чтобы быстро повысить степень безошибочности модели, просто запустите несколько моделей машинного обучения и найдите среди их результатов лучшие (маленький секрет специалистов по машинному обучению). Методики обучения ансамблей в некотором смысле автоматически выполняют задачи, часто возлагаемые на экспертов по конвейерам машинного обучения:\n",
    "выбор, сравнение и объединение результатов различных моделей машинного обучения. Основное преимущество обучения ансамблей — возможность выполнения его по отдельности для каждого значения данных во время выполнения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этой главе мы рассмотрели десять простых алгоритмов машинного обучения, необходимых для успешной работы в данной сфере. Вы посмотрели на предсказание значений с помощью алгоритмов регрессии, в частности, линейной, KNN и нейронных сетей. Вы также узнали об алгоритмах классификации: логистической регрессии, обучении с помощью деревьев принятия решений, SVM и случайных лесах. Более того, научились вычислять основные статистические показатели многомерных данных и использовать алгоритм k-средних для обучения без учителя. Эти алгоритмы и методы входят в число важнейших в сфере машинного обучения, и вам придется изучить еще очень много всего, чтобы стать специалистом по машинному обучению."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
